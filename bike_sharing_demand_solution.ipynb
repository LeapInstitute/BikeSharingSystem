{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Bike Sharing Demand with AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll tackle the Bike Sharing Demand competition from Kaggle using AutoGluon, a powerful AutoML library. The goal is to predict bike rental demand (count of total rentals) based on date, time, and weather features.\n",
    "\n",
    "Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded.\n",
    "\n",
    "Predicting bike sharing demand is highly relevant to related problems companies encounter, such as Uber, Lyft, and DoorDash. Accurate demand forecasting helps businesses prepare for spikes in their services and improves customer experience by limiting delays.\n",
    "\n",
    "We'll use AutoGluon's Tabular Prediction to fit data from CSV files provided by the competition. Through several iterations of model training and optimization, we'll aim to achieve a competitive score on the Kaggle leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install autogluon\n",
    "# !pip install kaggle\n",
    "# !pip install matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Explore the Dataset\n",
    "\n",
    "First, we need to download the competition data from Kaggle. If you haven't already set up your Kaggle API credentials, you'll need to do that first by creating a kaggle.json file in the ~/.kaggle/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('../BikeSharingSystem/data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download data using Kaggle API (uncomment if needed)\n",
    "# !kaggle competitions download -c bike-sharing-demand -p {data_dir}\n",
    "# !unzip -o {data_dir}/bike-sharing-demand.zip -d {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we'll assume the data files are already downloaded\n",
    "# Define file paths\n",
    "train_path = data_dir / 'train.csv'\n",
    "test_path = data_dir / 'test.csv'\n",
    "\n",
    "# If files don't exist, provide instructions\n",
    "if not train_path.exists() or not test_path.exists():\n",
    "    print(\"Data files not found. Please download from Kaggle and place in the data directory.\")\n",
    "    print(\"You can use the Kaggle API with: kaggle competitions download -c bike-sharing-demand\")\n",
    "else:\n",
    "    print(\"Data files found. Ready to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load and Examine the Data\n",
    "\n",
    "Let's load the training and test datasets and examine their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# Display basic information about the training data\n",
    "print(\"Training data shape:\", train.shape)\n",
    "print(\"\\nTraining data info:\")\n",
    "train.info()\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(\"\\nFirst 5 rows of training data:\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the test data\n",
    "print(\"Test data shape:\", test.shape)\n",
    "print(\"\\nTest data info:\")\n",
    "test.info()\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand the Features\n",
    "\n",
    "Let's understand what each feature represents:\n",
    "\n",
    "- **datetime**: hourly date + timestamp  \n",
    "- **season**: 1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- **holiday**: whether the day is a holiday or not\n",
    "- **workingday**: whether the day is neither a weekend nor holiday\n",
    "- **weather**: \n",
    "  - 1: Clear, Few clouds, Partly cloudy\n",
    "  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- **temp**: temperature in Celsius\n",
    "- **atemp**: \"feels like\" temperature in Celsius\n",
    "- **humidity**: relative humidity\n",
    "- **windspeed**: wind speed\n",
    "- **casual**: number of non-registered user rentals (only in train)\n",
    "- **registered**: number of registered user rentals (only in train)\n",
    "- **count**: number of total rentals (target variable, only in train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Statistical Summary of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of the training data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize the Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable (count)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['count'], bins=50, kde=True)\n",
    "plt.title('Distribution of Bike Rentals (Count)', fontsize=14)\n",
    "plt.xlabel('Number of Rentals', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Also plot the log-transformed target, which might be more normally distributed\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(train['count']), bins=50, kde=True)\n",
    "plt.title('Distribution of Log-Transformed Bike Rentals', fontsize=14)\n",
    "plt.xlabel('Log(Number of Rentals + 1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target variable is right-skewed, which is common for count data. The log-transformed version appears more normally distributed, suggesting that a log transformation might be beneficial for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Explore Relationships Between Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime to datetime type\n",
    "train['datetime'] = pd.to_datetime(train['datetime'])\n",
    "test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "\n",
    "# Extract hour from datetime\n",
    "train['hour'] = train['datetime'].dt.hour\n",
    "test['hour'] = test['datetime'].dt.hour\n",
    "\n",
    "# Plot bike rentals by hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='hour', y='count', data=train)\n",
    "plt.title('Bike Rentals by Hour of Day', fontsize=14)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Number of Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract day of week from datetime\n",
    "train['dayofweek'] = train['datetime'].dt.dayofweek\n",
    "test['dayofweek'] = test['datetime'].dt.dayofweek\n",
    "\n",
    "# Plot bike rentals by day of week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='dayofweek', y='count', data=train)\n",
    "plt.title('Bike Rentals by Day of Week', fontsize=14)\n",
    "plt.xlabel('Day of Week (0=Monday, 6=Sunday)', fontsize=12)\n",
    "plt.ylabel('Number of Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bike rentals by season\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='season', y='count', data=train)\n",
    "plt.title('Bike Rentals by Season', fontsize=14)\n",
    "plt.xlabel('Season (1=Spring, 2=Summer, 3=Fall, 4=Winter)', fontsize=12)\n",
    "plt.ylabel('Number of Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 4), ['Spring', 'Summer', 'Fall', 'Winter'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bike rentals by weather\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='weather', y='count', data=train)\n",
    "plt.title('Bike Rentals by Weather Condition', fontsize=14)\n",
    "plt.xlabel('Weather Condition', fontsize=12)\n",
    "plt.ylabel('Number of Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 4), ['Clear', 'Mist/Cloudy', 'Light Rain/Snow', 'Heavy Rain/Snow'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = train.drop('datetime', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Key Insights from Data Exploration\n",
    "\n",
    "From our exploratory data analysis, we can draw several insights:\n",
    "\n",
    "1. **Hourly Patterns**: There are clear peaks in bike rentals during commuting hours (8-9 AM and 5-6 PM), suggesting that many people use bikes for commuting to and from work.\n",
    "\n",
    "2. **Day of Week Patterns**: Weekdays show different patterns compared to weekends, with weekdays having more pronounced morning and evening peaks.\n",
    "\n",
    "3. **Seasonal Variation**: Bike rentals are higher in summer and fall, and lower in winter and spring, likely due to weather conditions.\n",
    "\n",
    "4. **Weather Impact**: Clear weather conditions (1) have higher rental counts compared to more severe weather conditions (3 and 4).\n",
    "\n",
    "5. **Temperature Correlation**: There's a positive correlation between temperature and bike rentals, indicating people are more likely to rent bikes in warmer weather.\n",
    "\n",
    "6. **Registered vs. Casual Users**: Registered users make up a larger portion of the rentals compared to casual users.\n",
    "\n",
    "These insights will guide our feature engineering process to create meaningful features that capture these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "Based on our exploratory analysis, we'll create additional features to help the model capture important patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"Apply feature engineering to the dataframe\"\"\"\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert datetime to datetime type if it's not already\n",
    "    if df['datetime'].dtype != 'datetime64[ns]':\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Extract datetime components\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day'] = df['datetime'].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Create time of day categories\n",
    "    df['time_of_day'] = df['hour'].apply(lambda x: \n",
    "                                        'morning' if 6 <= x < 12 else\n",
    "                                        'afternoon' if 12 <= x < 18 else\n",
    "                                        'evening' if 18 <= x < 22 else\n",
    "                                        'night')\n",
    "    \n",
    "    # Create rush hour flag (7-9 AM and 4-7 PM on weekdays)\n",
    "    df['is_rush_hour'] = (((df['hour'] >= 7) & (df['hour'] <= 9) | \n",
    "                          (df['hour'] >= 16) & (df['hour'] <= 19)) & \n",
    "                          (df['dayofweek'] < 5)).astype(int)\n",
    "    \n",
    "    # Create weekend flag\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # Combine weather and season for more context\n",
    "    df['weather_season'] = df['weather'].astype(str) + '_' + df['season'].astype(str)\n",
    "    \n",
    "    # Create temperature bins\n",
    "    df['temp_bin'] = pd.cut(df['temp'], bins=[-20, 0, 10, 20, 30, 50], \n",
    "                           labels=['very_cold', 'cold', 'mild', 'warm', 'hot'])\n",
    "    \n",
    "    # Create humidity bins\n",
    "    df['humidity_bin'] = pd.cut(df['humidity'], bins=[0, 25, 50, 75, 100], \n",
    "                              labels=['low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Create windspeed bins\n",
    "    df['windspeed_bin'] = pd.cut(df['windspeed'], bins=[0, 10, 20, 30, 100], \n",
    "                               labels=['low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Create interaction features\n",
    "    df['temp_humidity'] = df['temp'] * df['humidity']\n",
    "    df['temp_windspeed'] = df['temp'] * df['windspeed']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to both train and test sets\n",
    "train_fe = feature_engineering(train)\n",
    "test_fe = feature_engineering(test)\n",
    "\n",
    "# Display the new features\n",
    "print(\"Training data with engineered features:\")\n",
    "train_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Data for AutoGluon\n",
    "\n",
    "Now that we've created additional features, let's prepare the data for AutoGluon. We'll need to:\n",
    "1. Remove the original datetime column (since we've extracted its components)\n",
    "2. Convert categorical features to the appropriate type\n",
    "3. Create a log-transformed version of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the datetime column\n",
    "train_fe = train_fe.drop('datetime', axis=1)\n",
    "test_fe = test_fe.drop('datetime', axis=1)\n",
    "\n",
    "# Convert categorical features to category type\n",
    "categorical_features = ['season', 'holiday', 'workingday', 'weather', 'time_of_day', \n",
    "                        'temp_bin', 'humidity_bin', 'windspeed_bin', 'weather_season']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    train_fe[feature] = train_fe[feature].astype('category')\n",
    "    test_fe[feature] = test_fe[feature].astype('category')\n",
    "\n",
    "# Create a log-transformed version of the target variable\n",
    "train_fe['count_log'] = np.log1p(train_fe['count'])\n",
    "\n",
    "# Display the prepared data\n",
    "print(\"Training data prepared for AutoGluon:\")\n",
    "train_fe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initial Model Training with AutoGluon\n",
    "\n",
    "Let's train an initial model using AutoGluon with default settings to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to use for training\n",
    "features = [col for col in train_fe.columns if col not in ['casual', 'registered', 'count', 'count_log']]\n",
    "\n",
    "models_dir = Path('BikeSharingSystem/models')\n",
    "models_dir.mkdir(parents=True, exist_ok=True)  # Add parents=True to create parent directories\n",
    "\n",
    "# Create TabularDataset for AutoGluon\n",
    "train_data = TabularDataset(train_fe[features + ['count']])\n",
    "\n",
    "# Train initial model with default settings\n",
    "initial_predictor = TabularPredictor(label='count', eval_metric='root_mean_squared_error',path=str(models_dir / 'initial_model')\n",
    "initial_predictor.fit(train_data=train_data, time_limit=600)  # 10 minutes time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the initial model\n",
    "initial_leaderboard = initial_predictor.leaderboard()\n",
    "print(\"Initial model leaderboard:\")\n",
    "initial_leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate Initial Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "test_data = TabularDataset(test_fe[features])\n",
    "initial_predictions = initial_predictor.predict(test_data)\n",
    "\n",
    "# Create submission file\n",
    "initial_submission = pd.DataFrame({\n",
    "    'datetime': test['datetime'].astype(str),\n",
    "    'count': initial_predictions\n",
    "})\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "initial_submission['count'] = initial_submission['count'].clip(lower=0)\n",
    "\n",
    "# Save submission file\n",
    "initial_submission_path = data_dir / 'initial_submission.csv'\n",
    "initial_submission.to_csv(initial_submission_path, index=False)\n",
    "\n",
    "print(f\"Initial submission file saved to {initial_submission_path}\")\n",
    "initial_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Optimization - Iteration 1 (Log Transform)\n",
    "\n",
    "For our first optimization, we'll train a model using the log-transformed target variable, which is often more appropriate for count data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TabularDataset with log-transformed target\n",
    "train_data_log = TabularDataset(train_fe[features + ['count_log']])\n",
    "\n",
    "# Train model with log-transformed target\n",
    "log_predictor = TabularPredictor(label='count_log', eval_metric='root_mean_squared_error')\n",
    "log_predictor.fit(train_data=train_data_log, time_limit=600)  # 10 minutes time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the log-transformed model\n",
    "log_leaderboard = log_predictor.leaderboard()\n",
    "print(\"Log-transformed model leaderboard:\")\n",
    "log_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "log_predictions = log_predictor.predict(test_data)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "log_predictions_original_scale = np.expm1(log_predictions)\n",
    "\n",
    "# Create submission file\n",
    "log_submission = pd.DataFrame({\n",
    "    'datetime': test['datetime'].astype(str),\n",
    "    'count': log_predictions_original_scale\n",
    "})\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "log_submission['count'] = log_submission['count'].clip(lower=0)\n",
    "\n",
    "# Save submission file\n",
    "log_submission_path = data_dir / 'log_submission.csv'\n",
    "log_submission.to_csv(log_submission_path, index=False)\n",
    "\n",
    "print(f\"Log-transformed model submission file saved to {log_submission_path}\")\n",
    "log_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Optimization - Iteration 2 (Advanced Configuration)\n",
    "\n",
    "For our second optimization, we'll use more advanced AutoGluon configurations, including:\n",
    "1. Using the 'best_quality' preset for more thorough model training\n",
    "2. Implementing bagging with multiple folds\n",
    "3. Using stacking to combine multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with advanced configuration\n",
    "advanced_predictor = TabularPredictor(label='count_log', eval_metric='root_mean_squared_error')\n",
    "advanced_predictor.fit(\n",
    "    train_data=train_data_log,\n",
    "    time_limit=1200,  # 20 minutes time limit\n",
    "    presets='best_quality',  # Use more advanced models and tuning\n",
    "    num_bag_folds=5,  # Use bagging for better performance\n",
    "    num_stack_levels=1  # Use stacking for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the advanced model\n",
    "advanced_leaderboard = advanced_predictor.leaderboard()\n",
    "print(\"Advanced model leaderboard:\")\n",
    "advanced_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "advanced_predictions = advanced_predictor.predict(test_data)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "advanced_predictions_original_scale = np.expm1(advanced_predictions)\n",
    "\n",
    "# Create submission file\n",
    "advanced_submission = pd.DataFrame({\n",
    "    'datetime': test['datetime'].astype(str),\n",
    "    'count': advanced_predictions_original_scale\n",
    "})\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "advanced_submission['count'] = advanced_submission['count'].clip(lower=0)\n",
    "\n",
    "# Save submission file\n",
    "advanced_submission_path = data_dir / 'advanced_submission.csv'\n",
    "advanced_submission.to_csv(advanced_submission_path, index=False)\n",
    "\n",
    "print(f\"Advanced model submission file saved to {advanced_submission_path}\")\n",
    "advanced_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features were most important for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best model\n",
    "feature_importance = advanced_predictor.feature_importance(test_data)\n",
    "print(\"Feature importance:\")\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Comparison and Final Submission\n",
    "\n",
    "Let's compare the performance of our three models and select the best one for our final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "print(\"Initial Model Performance:\")\n",
    "print(initial_leaderboard.head(1))\n",
    "\n",
    "print(\"\\nLog-Transformed Model Performance:\")\n",
    "print(log_leaderboard.head(1))\n",
    "\n",
    "print(\"\\nAdvanced Model Performance:\")\n",
    "print(advanced_leaderboard.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final submission file (using the advanced model)\n",
    "final_submission = advanced_submission.copy()\n",
    "final_submission_path = data_dir / 'final_submission.csv'\n",
    "final_submission.to_csv(final_submission_path, index=False)\n",
    "\n",
    "print(f\"Final submission file saved to {final_submission_path}\")\n",
    "final_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary and Conclusion\n",
    "\n",
    "In this project, we tackled the Bike Sharing Demand competition using AutoGluon. Here's a summary of our approach and findings:\n",
    "\n",
    "### Data Exploration and Insights\n",
    "- We found clear patterns in bike rentals based on time of day, day of week, season, and weather conditions.\n",
    "- Commuting hours (8-9 AM and 5-6 PM) showed peak rental activity.\n",
    "- Weather and temperature had significant impacts on rental patterns.\n",
    "\n",
    "### Feature Engineering\n",
    "- We extracted datetime components (hour, day, month, year, day of week).\n",
    "- Created categorical features like time of day, temperature bins, and humidity bins.\n",
    "- Added interaction features combining weather and seasonal information.\n",
    "- Created flags for rush hours and weekends.\n",
    "\n",
    "### Model Development\n",
    "1. **Initial Model**: Trained with default settings as a baseline.\n",
    "2. **Log-Transformed Model**: Applied log transformation to the target variable to better handle the count data distribution.\n",
    "3. **Advanced Model**: Used best_quality preset with bagging and stacking for improved performance.\n",
    "\n",
    "### Key Findings\n",
    "- Log transformation of the target variable significantly improved model performance.\n",
    "- The most important features were hour of day, temperature, and season.\n",
    "- Advanced configurations with bagging and stacking further improved model performance.\n",
    "\n",
    "### Future Improvements\n",
    "- Incorporate external data like public holidays or events that might affect bike rentals.\n",
    "- Experiment with more complex feature interactions.\n",
    "- Try time series specific models to better capture temporal patterns.\n",
    "- Implement hyperparameter tuning for specific models within AutoGluon.\n",
    "\n",
    "This project demonstrates the power of AutoGluon for quickly developing high-performing models with minimal manual tuning, while still allowing for sophisticated feature engineering and model optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
